"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[3601],{5301(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA) Systems","description":"Welcome to Module 4 of the Physical AI & Humanoid Robotics Textbook. This module integrates perception, language, and action into a coherent embodied AI system and concludes the curriculum with the autonomous humanoid architecture.","source":"@site/docs/module-4-vla/index.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/Hackathons/docs/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/muslim785/Hackathons/tree/main/docs/module-4-vla/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docs","previous":{"title":"Module 3: RAG-Friendly Content Structure","permalink":"/Hackathons/docs/module-3/rag-structure"},"next":{"title":"Chapter 1: VLA Foundations - Understanding Vision-Language-Action Systems","permalink":"/Hackathons/docs/module-4-vla/chapter-1-vla-foundations"}}');var o=i(4848),s=i(8453);const r={},l="Module 4: Vision-Language-Action (VLA) Systems",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Path",id:"learning-path",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Guiding Principle",id:"guiding-principle",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla-systems",children:"Module 4: Vision-Language-Action (VLA) Systems"})}),"\n",(0,o.jsx)(n.p,{children:"Welcome to Module 4 of the Physical AI & Humanoid Robotics Textbook. This module integrates perception, language, and action into a coherent embodied AI system and concludes the curriculum with the autonomous humanoid architecture."}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"This module teaches Vision-Language-Action (VLA) systems conceptually, focusing on how cognition emerges in embodied systems rather than just connecting LLMs to robots. You'll learn about:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"How vision, language, and action work together in embodied robots"}),"\n",(0,o.jsx)(n.li,{children:"Voice-to-action pipelines for human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Cognitive planning with LLMs as planners (not controllers)"}),"\n",(0,o.jsx)(n.li,{children:"Complete autonomous humanoid architecture"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-path",children:"Learning Path"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chapter 1: VLA Foundations"})," - Understanding Vision-Language-Action systems conceptually"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chapter 2: Voice-to-Action Pipelines"})," - Speech as robot control interface"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chapter 3: Cognitive Planning with LLMs"})," - How LLMs function as planners rather than controllers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chapter 4: Autonomous Humanoid Architecture"})," - Complete end-to-end system architecture"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understanding of ROS 2, simulation, and robot perception (Modules 1-3)"}),"\n",(0,o.jsx)(n.li,{children:"Basic familiarity with Large Language Model concepts"}),"\n",(0,o.jsx)(n.li,{children:"Knowledge of robot control and navigation fundamentals"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"guiding-principle",children:"Guiding Principle"}),"\n",(0,o.jsxs)(n.p,{children:["Remember: ",(0,o.jsx)(n.strong,{children:"LLMs perform cognition and planning; ROS 2 performs execution. Never blur this line."})]}),"\n",(0,o.jsx)(n.p,{children:"This fundamental principle separates high-level reasoning (cognition) from low-level execution (action), ensuring system reliability while leveraging AI capabilities."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);