"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[7121],{4281(e,n,t){t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/terminology","title":"Terminology Reference: Vision-Language-Action (VLA) Systems","description":"This document defines key terms used throughout Module 4 to ensure consistency across all chapters.","source":"@site/docs/module-4-vla/terminology.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/terminology","permalink":"/Hackathons/docs/module-4-vla/terminology","draft":false,"unlisted":false,"editUrl":"https://github.com/muslim785/Hackathons/tree/main/docs/module-4-vla/terminology.md","tags":[],"version":"current","frontMatter":{}}');var o=t(4848),s=t(8453);const a={},c="Terminology Reference: Vision-Language-Action (VLA) Systems",r={},l=[{value:"Core Terms",id:"core-terms",level:2},{value:"VLA (Vision-Language-Action)",id:"vla-vision-language-action",level:3},{value:"Intent",id:"intent",level:3},{value:"Planner",id:"planner",level:3},{value:"Action Sequence",id:"action-sequence",level:3},{value:"Embodiment",id:"embodiment",level:3},{value:"Cognitive Planning",id:"cognitive-planning",level:3},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:3},{value:"Autonomous Humanoid Architecture",id:"autonomous-humanoid-architecture",level:3},{value:"Important Distinctions",id:"important-distinctions",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"terminology-reference-vision-language-action-vla-systems",children:"Terminology Reference: Vision-Language-Action (VLA) Systems"})}),"\n",(0,o.jsx)(n.p,{children:"This document defines key terms used throughout Module 4 to ensure consistency across all chapters."}),"\n",(0,o.jsx)(n.h2,{id:"core-terms",children:"Core Terms"}),"\n",(0,o.jsx)(n.h3,{id:"vla-vision-language-action",children:"VLA (Vision-Language-Action)"}),"\n",(0,o.jsx)(n.p,{children:"An integrated system connecting vision (perception), language (cognition), and action (physical execution) in embodied robots."}),"\n",(0,o.jsx)(n.h3,{id:"intent",children:"Intent"}),"\n",(0,o.jsx)(n.p,{children:"The interpreted purpose or goal extracted from a natural language command, processed by the cognitive planner to generate action sequences."}),"\n",(0,o.jsx)(n.h3,{id:"planner",children:"Planner"}),"\n",(0,o.jsx)(n.p,{children:"An LLM-based component that translates high-level goals into executable action sequences considering physical constraints. The planner is distinct from controllers that execute actions."}),"\n",(0,o.jsx)(n.h3,{id:"action-sequence",children:"Action Sequence"}),"\n",(0,o.jsx)(n.p,{children:"A series of executable steps generated by the cognitive planner to achieve a specific goal, respecting physical constraints and safety requirements."}),"\n",(0,o.jsx)(n.h3,{id:"embodiment",children:"Embodiment"}),"\n",(0,o.jsx)(n.p,{children:"The concept that AI systems behave differently when physically situated in the real world, as opposed to disembodied systems. Embodied systems must account for physical laws, sensor noise, actuator limitations, and safety considerations."}),"\n",(0,o.jsx)(n.h3,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,o.jsx)(n.p,{children:"The process by which LLMs function as high-level planners that generate task plans, rather than directly controlling robot actuators. This maintains separation between cognition and execution."}),"\n",(0,o.jsx)(n.h3,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"A processing chain converting spoken commands to robotic actions through speech recognition, intent mapping, and safety validation before action execution."}),"\n",(0,o.jsx)(n.h3,{id:"autonomous-humanoid-architecture",children:"Autonomous Humanoid Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The complete system design integrating perception \u2192 cognition \u2192 action flow for independent robot operation, incorporating all VLA system components."}),"\n",(0,o.jsx)(n.h2,{id:"important-distinctions",children:"Important Distinctions"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planner vs. Controller"}),": LLMs perform cognition and planning; ROS 2 performs execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Embodied vs. Disembodied"}),": Physical constraints fundamentally alter how AI systems behave"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Conceptual vs. Implementation"}),": Focus on system-level understanding rather than technical implementation details"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>c});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);