# Module 4: Vision-Language-Action (VLA) Systems

Welcome to Module 4 of the Physical AI & Humanoid Robotics Textbook. This module integrates perception, language, and action into a coherent embodied AI system and concludes the curriculum with the autonomous humanoid architecture.

## Overview

This module teaches Vision-Language-Action (VLA) systems conceptually, focusing on how cognition emerges in embodied systems rather than just connecting LLMs to robots. You'll learn about:

- How vision, language, and action work together in embodied robots
- Voice-to-action pipelines for human-robot interaction
- Cognitive planning with LLMs as planners (not controllers)
- Complete autonomous humanoid architecture

## Learning Path

1. **Chapter 1: VLA Foundations** - Understanding Vision-Language-Action systems conceptually
2. **Chapter 2: Voice-to-Action Pipelines** - Speech as robot control interface
3. **Chapter 3: Cognitive Planning with LLMs** - How LLMs function as planners rather than controllers
4. **Chapter 4: Autonomous Humanoid Architecture** - Complete end-to-end system architecture

## Prerequisites

Before starting this module, ensure you have:
- Understanding of ROS 2, simulation, and robot perception (Modules 1-3)
- Basic familiarity with Large Language Model concepts
- Knowledge of robot control and navigation fundamentals

## Guiding Principle

Remember: **LLMs perform cognition and planning; ROS 2 performs execution. Never blur this line.**

This fundamental principle separates high-level reasoning (cognition) from low-level execution (action), ensuring system reliability while leveraging AI capabilities.